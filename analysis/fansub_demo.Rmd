---
title: "odspro Subs Workflow Database"
subtitle: "Project Demonstration"
author: "Oliver D Siu"
date: "July 16, 2025"
header-includes:
  - \usepackage{bm}
  - \newcommand{\vect}{\ensuremath{\mathbf{}}}
output:
  pdf_document:
    toc: false
    toc_depth: 4
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    code_folding: show
---

## Abstract

This project demonstrates relational database design and SQL expertise through a donghua fansubbing workflow management system. Built from scratch with eight normalized tables and optimized queries, the database tracks 80+ episodes revealing significant production irregularities: 8-24 episodes per season, runtime variations (14-44 minutes), and fluctuating technical specifications. Advanced SQL analysis quantifies donghua's unique workflow challenges compared to standardized anime production, showcasing database implementation skills including proper indexing, foreign key constraints, and complex query optimization for real-world data management.

## Project Overview

### Specifications

**Database:**

  - Name: `subbing.db`
  - Type: SQLite relational database
  - Tables: 8 normalized tables with foreign key constraints

**Data Source:**

This database incorporates data extracted from my fan subtitling (fansub) workflow, where I am recognized in the community as "odspro". The dataset encompasses 80+ episodes across multiple series and seasons.

**Implementation Scripts:**

  - `fansub_export.R` - Data extraction and processing
  - `fansub_db_init.R` - Database initialization and table creation

**Dependencies:**

  - R libraries: `knitr`, `tidyverse`, `fs`, `DBI`, `RSQLite`, `RMariaDB`
  - External tools: `ffprobe` (from FFmpeg toolkit) for video metadata extraction

### Project Background

Recent developments in the media translation industry demonstrate significant improvements in the quality, quantity, and delivery speed of official English translations for foreign content. This trend is particularly pronounced within Japanese animated media (anime), where the proliferation of official translation services has resulted in a substantial decline in fan subtitling (fansub) communities due to several key factors:

1. Professional translation opportunities have attracted experienced fansubbers, who leverage their community expertise for commercial roles.
2. The accessibility and affordability of official translations provide consumers with convenient legal alternatives to fan-produced content.
3. Expectations of quality have risen, requiring fansubbers to demonstrate clear superiority over official translations to maintain relevance.

This third factor manifests across multiple aspects. Beyond enhanced translation quality, fan subtitles must incorporate advanced technical features including song lyric translations, sign translations, and karaoke animations. These elevated standards are sustainable within the anime community where official subtitles can supplement fansubbing workflows, but the Chinese animation (donghua) ecosystem lacks comparable infrastructure.

The donghua translation landscape presents distinct challenges. A majority of donghua remains without official English translations, and existing translations frequently exhibit quality issues, often relying on machine translation with minimal human oversight. Some platforms explicitly acknowledge their use of automated translation systems without review processes. This translation gap has generated renewed demand for fansubbers, yet quality expectations from the anime community have transferred to donghua despite the absence of supporting infrastructure. These elevated standards create significant barriers for newcomers, as sustainable fansubbing typically requires collaborative teams to distribute specialized tasks such as video encoding, multiplexing, timing, typesetting, styling, subtitle animation, and related technical processes. Consequently, solo fansubbers must simultaneously manage translation responsibilities alongside comprehensive technical production workflows.

This project demonstrates the multifaceted workflow management challenges inherent in donghua fansubbing operations. A relational database was selected based on the structured characteristics of the production workflow. Experience from anime fansubbing workflows provides a foundation for consistent operational patterns which Chinese animation productions also follow. This standardization creates data consistency that is well-suited for relational database implementation.

\pagebreak

### Schema

![Database Schema Diagram](fansub_schema.png)

*Note:*  
- (PK) indicates Primary Key  
- (FK) indicates Foreign Key  

**Tables:**

series  
- `series_id` (PK)  
- `title` 

seasons  
- `season_id` (PK)  
- `series_id` (FK to `series.series_id`)  
- `season_number`  

episodes  
- `episode_id` (PK)  
- `season_id` (FK to `seasons.season_id`)  
- `series_id` (FK to `series.series_id`)  
- `episode_number`  

statuses  
- `status_id` (PK)  
- `status_type`  
- `status_name`  

status_history  
- `history_id` (PK)  
- `file_id` (FK to `files.file_id`)  
- `status_id` (FK to `statuses.status_id`)  
- `changed_at`

files  
- `file_id` (PK)  
- `file_type`  
- `file_name`  
- `episode_id` (FK to `episodes.episode_id`)  

video_metadata  
- `file_id` (PK)  
- `container`  
- `video_codec`  
- `audio_codec`  
- `resolution_x`  
- `resolution_y`  
- `duration`  

subtitle_metadata  
- `file_id` (PK)  
- `line_count`  

script_metadata  
- `file_id` (PK)  
- `line_count`  

**Relationships:**  
- One series has many seasons  
- One season has many episodes  
- One series has many episodes  
- One episode has many files  
- One status can have many `status_history` entries  
- One file can have many `status_history` entries  
- Each file can have only one `video_metadata` or `subtitle_metadata` or `script_metadata` entry  

\pagebreak

## Sample Queries

```{r, message = FALSE, include = FALSE}
library(knitr)
library(tidyverse, warn.conflicts = FALSE)
library(RMariaDB)
library(DBI)
```

```{r, include = FALSE}
con <- dbConnect(
  RSQLite::SQLite(),
  dbname = "subbing.db"
)
```

### Query 1 -- Which episodes have not been subtitled?

```{sql, connection = con, output.var = "missing_sub"}
-- Query to count total episodes and missing subtitles by series and season
-- Uses LEFT JOIN to identify episodes without subtitle files
SELECT
    s.title AS series,
    se.season_number,
    COUNT(e.episode_id) AS total_episodes,
    COUNT(
        CASE
            WHEN f.file_id IS NULL THEN 1
        END
    ) AS missing_subtitles
FROM episodes AS e
JOIN seasons AS se
    ON e.season_id = se.season_id
JOIN series AS s
    ON se.series_id = s.series_id
LEFT JOIN files AS f
    ON f.episode_id = e.episode_id AND f.file_type = 'subtitle'
GROUP BY s.title, se.season_number
ORDER BY s.title, se.season_number;
```

```{r, echo = FALSE}
sub_start <-
  missing_sub |>
  mutate(
    started = total_episodes - missing_subtitles,
    ss_label = paste0(series, " S", season_number)
  ) |>
  select(ss_label, started, missing = missing_subtitles) |>
  pivot_longer(
    cols = c(started, missing),
    names_to = "status",
    values_to = "count"
  )

sub_start |>
  ggplot(aes(x = ss_label, y = count, fill = status)) +
  geom_bar(stat = "identity") +
  labs(
    x = "Series and Season",
    y = "Episode Count",
    title = "Episodes Started",
    fill = "Status"
  ) +
  scale_fill_manual(
    values = c("started" = "royalblue", "missing" = "goldenrod"),
    labels = c(started = "Started", missing = "Missing")
  ) +
  scale_y_continuous(breaks = seq(0, 24, by = 4)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
```

The bar chart visualizes the distribution of episodes with and without subtitle file initialization across series and seasons. The analysis reveals that The Outcast Season 2 exhibits substantial missing subtitle initialization. This large amount of missing data may impact subsequent query calculations. Episode counts per season demonstrate considerable variability. A typical season contains 12 episodes, while Season 2 contains 24 episodes and Season 3 contains 8 episodes. These variations indicate inconsistent production scheduling patterns.

### Query 2 -- Which are the 10 most recent status updates for subtitle files?

```{sql, connection = con, output.var = "subpage_updates"}
-- Query to get the latest status updates for subtitle files
-- Uses CTE with ROW_NUMBER() to find most recent status per file
WITH latest_status AS (
    SELECT
        f.file_id,
        st.status_type,
        st.status_name,
        sh.changed_at,
        f.file_type,
        ROW_NUMBER() OVER (
            PARTITION BY f.file_id
            ORDER BY sh.changed_at DESC
        ) AS rn
    FROM files AS f
    JOIN status_history AS sh
        ON f.file_id = sh.file_id
    JOIN statuses AS st
        ON sh.status_id = st.status_id
    WHERE f.file_type = 'subtitle'
)
SELECT
    s.title AS series,
    se.season_number,
    e.episode_number,
    ls.file_type AS table_source,
    ls.status_type,
    ls.status_name,
    ls.changed_at
FROM latest_status AS ls
JOIN episodes AS e
    ON ls.file_id = e.episode_id
JOIN seasons AS se
    ON e.season_id = se.season_id
JOIN series AS s
    ON se.series_id = s.series_id
WHERE ls.rn = 1
ORDER BY s.title, se.season_number, e.episode_number
LIMIT 10;
```

```{r, echo = FALSE}
colnames(subpage_updates) <- c(
  "Series",
  "Season",
  "Episode",
  "File Type",
  "Status Type",
  "Status",
  "Changed At"
)
subpage_updates |> head(10) |> kable(caption = "10 Most Recent Status Updates")
```

This query returns commonly requested information regarding subtitle file status tracking. SQL backends are frequently implemented to support web applications by offloading computational tasks from client devices. The development of this query was motivated by documented incidents where fansubbing organizations received criticism for lacking such informational capabilities on their platforms.

### Query 3 -- Has video resolution increased with each season of The Outcast?

```{sql, connection = con, output.var = "video_resolution"}
-- Query to find the highest video resolution for each season of 'The Outcast'
-- Uses CTE with ROW_NUMBER() to rank resolutions by total pixel count
WITH season_resolutions AS (
    SELECT
        s.title AS series,
        se.season_number,
        vm.resolution_x,
        vm.resolution_y,
        (vm.resolution_x * vm.resolution_y) AS resolution_total,
        ROW_NUMBER() OVER (
            PARTITION BY se.season_number
            ORDER BY (vm.resolution_x * vm.resolution_y) DESC
        ) AS rn
    FROM series AS s
    JOIN seasons AS se
        ON s.series_id = se.series_id
    JOIN episodes AS e
        ON se.season_id = e.season_id
    JOIN files AS f
        ON e.episode_id = f.episode_id AND f.file_type = 'video'
    JOIN video_metadata AS vm
        ON f.file_id = vm.file_id
    WHERE s.title = 'The Outcast'
)
SELECT
    series,
    season_number,
    resolution_x,
    resolution_y,
    resolution_total
FROM season_resolutions
WHERE rn = 1
ORDER BY season_number;
```

```{r, echo = FALSE}
resolution_time <-
  video_resolution |>
  select(
    "Season" = season_number,
    "X" = resolution_x,
    "Y" = resolution_y,
    "Total" = resolution_total
  ) |>
  pivot_longer(
    cols = c(X, Y, Total),
    names_to = "Type",
    values_to = "Pixels"
  ) |>
  mutate(
    pixels_scaled = ifelse(Type == "Total", Pixels / 1000, Pixels)
  )

resolution_time |>
  ggplot(aes(x = Season, y = pixels_scaled, color = Type, linetype = Type, shape = Type)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(
    name = "X/Y (Pixels)",
    sec.axis = sec_axis(~ . * 1000, name = "Total (Pixels)")
  ) +
  labs(
    title = "Video Resolution Types by Season for The Outcast",
    x = "Season",
    color = "Type"
  ) +
  scale_color_manual(values = c(
    "X" = "dodgerblue",
    "Y" = "forestgreen",
    "Total" = "firebrick"
  )) +
  theme_minimal()
```

The visualization displays video resolution metrics across seasons for The Outcast series. The analysis examines peak resolution values for each season due to internal resolution variations from inconsistent production quality. Video resolution has not demonstrated consistent improvement across seasons. Maximum pixel resolution of 1920x1080 was achieved in Seasons 2 and 3. Subsequent seasons maintained 1920 pixel horizontal resolution while reducing vertical resolution for artistic ultrawide aspect ratios, which compromised overall pixel count.

### Query 4 --  Is there a correlation between subtitle line counts and video durations?

```{sql, connection = con, output.var = "subtitle_video"}
-- Query to compare subtitle line counts with video durations
-- Uses two CTEs to join subtitle and video metadata, converts duration to seconds
WITH
    subtitle AS (
        SELECT
            e.episode_id,
            sm.line_count
        FROM episodes AS e
        JOIN files AS f
            ON e.episode_id = f.episode_id AND f.file_type = 'subtitle'
        JOIN subtitle_metadata AS sm
            ON f.file_id = sm.file_id
    ),
    video AS (
        SELECT
            e.episode_id,
            vm.duration
        FROM episodes AS e
        JOIN files f
            ON e.episode_id = f.episode_id AND f.file_type = 'video'
        JOIN video_metadata vm
            ON f.file_id = vm.file_id
    )
SELECT
    s.title AS series,
    se.season_number,
    e.episode_number,
    subtitle.line_count AS subtitle_lines,
    (CAST(SUBSTR(video.duration, 1, 2) AS INTEGER) * 3600 +
     CAST(SUBSTR(video.duration, 4, 2) AS INTEGER) * 60 +
     CAST(SUBSTR(video.duration, 7, 2) AS INTEGER)) AS video_duration_seconds
FROM series AS s
JOIN seasons AS se
    ON s.series_id = se.series_id
JOIN episodes AS e
    ON se.season_id = e.season_id
JOIN subtitle
    ON e.episode_id = subtitle.episode_id
LEFT JOIN video
    ON e.episode_id = video.episode_id
ORDER BY s.title, se.season_number, e.episode_number;
```

```{r, echo = FALSE}
subtitle_video_long <-
  subtitle_video |>
  select(
    series,
    season_number,
    episode_number,
    "Subtitle Lines" = subtitle_lines,
    "Duration (seconds)" = video_duration_seconds
  ) |>
  pivot_longer(
    cols = c("Subtitle Lines", "Duration (seconds)"),
    names_to = "measure",
    values_to = "value"
  ) |>
  mutate(ss = paste(series, paste0("S", season_number)))

subtitle_video_long |> 
  ggplot(aes(x = ss, y = value, fill = measure)) +
  geom_boxplot(position = position_dodge(), size = 0.3) +
  facet_wrap(~measure, scales = "free_y") +
  labs(
    title = "Video Duration and Subtitle Lines by Series and Season",
    x = "Series & Season",
    y = "Value",
    fill = "Measure"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  ) +
  scale_fill_manual(
    values = c(
      "Subtitle Lines" = "paleturquoise",
      "Duration (seconds)" = "thistle"
    )
  )
```

The boxplot analysis examines the distribution of subtitle line counts and video duration across series and seasons. While The Outcast Season 1 typically exhibits the highest subtitle line counts, its video duration remains comparatively shorter than other seasons. This discrepancy may result from production changes in later seasons, which incorporated baked advertisements into video files, thereby extending runtime. To Be Hero demonstrates significantly shorter runtime compared to The Outcast, yet maintains comparable subtitle line density. This pattern may reflect differences in genre and show styles.

```{r, echo = FALSE}
subtitle_video_long |>
  ggplot(aes(
    x = episode_number,
    y = value,
    color = ss,
    linetype = measure,
    shape = ss,
    group = interaction(ss, measure)
  )) +
  geom_line() +
  geom_point() +
  scale_y_continuous(
    name = "Subtitle Lines ",
    sec.axis = sec_axis(~ ., name = "Duration (seconds)")
  ) +
  labs(
    x = "Episode Number",
    color = "Series and Season",
    linetype = "Measure",
    shape = "Series and Season",
    title = "Subtitle Lines and Video Duration by Episode"
  ) +
  scale_x_continuous(breaks = seq(0, 12, by = 2)) +
  scale_color_brewer(palette = "Set2") +
  theme_minimal()
```

The analysis reveals distinct patterns in runtime and subtitle density throughout each season's progression. The Outcast Seasons 1 and 5 demonstrate extended runtime for season finales, while Season 4 concludes with the shortest episode duration in the entire series. To Be Hero Season 1 similarly implements extended runtime for its season finale, suggesting a common production strategy across different series.

### Query 5 -- How long are the episodes of The Outcast? Are there any outliers?

```{sql, connection = con, output.var = "eps_duration"}
-- Query to analyze episode durations for 'The Outcast' series
-- Converts HH:MM:SS duration format to seconds for easier analysis
SELECT
    se.season_number,
    e.episode_number,
    vm.duration,
    (CAST(SUBSTR(vm.duration, 1, 2) AS INTEGER) * 3600 +
     CAST(SUBSTR(vm.duration, 4, 2) AS INTEGER) * 60 +
     CAST(SUBSTR(vm.duration, 7, 2) AS FLOAT)) AS duration_seconds
FROM series AS s
JOIN seasons AS se
    ON s.series_id = se.series_id
JOIN episodes AS e
    ON se.season_id = e.season_id
JOIN files AS f
    ON e.episode_id = f.episode_id AND f.file_type = 'video'
JOIN video_metadata AS vm
    ON f.file_id = vm.file_id
WHERE s.title = 'The Outcast'
ORDER BY duration_seconds DESC;
```

```{r, echo = FALSE}
eps_duration |>
  ggplot(aes(x = duration_seconds)) +
  geom_histogram(bins = 22, fill = "lightblue", color = "black", linewidth = 0.3) +
  labs(
    title = "Histogram of Episode Durations for The Outcast",
    x = "Duration (seconds)",
    y = "Count"
  ) +
  theme_minimal()
```

The analysis of The Outcast episode durations reveals a concentration around 1400-1500 seconds (approximately 24 minutes). The distribution exhibits slight right-skewness with two notable outliers identified in the analysis.

```{r, echo = FALSE}
colnames(eps_duration) <- c("Season", "Episode", "Duration (HH:MM:SS)", "Duration (seconds)")
eps_duration |> head(5) |> kable(caption = "Top 5 Longest Episodes")
eps_duration |> tail(5) |> kable(caption = "Top 5 Shortest Episodes")
```

The outliers are identified as Season 1 Episode 12 with an extended 43:48 runtime and Season 4 Episode 12 with a condensed 14:54 runtime. Season 4 Episode 12 represents the shortest episode in the series despite containing embedded advertisements.

### Query 6 -- When did episodes receive their first revision? How long did it take?

```{sql, connection = con, output.var = "episode_status"}
-- Query to calculate time between episode completion and first revision
-- Uses multiple CTEs and window functions to track status changes over time
WITH episode_status_times AS (
    SELECT
        f.episode_id,
        e.episode_number,
        se.season_number,
        s.title AS series,
        sh.changed_at,
        st.status_type,
        ROW_NUMBER() OVER (
            PARTITION BY f.episode_id, st.status_type
            ORDER BY sh.changed_at DESC
        ) AS rn_all,
        ROW_NUMBER() OVER (
            PARTITION BY f.episode_id, st.status_type
            ORDER BY sh.changed_at
        ) AS rn_revision
    FROM status_history AS sh
    JOIN statuses AS st
        ON sh.status_id = st.status_id
    JOIN files AS f
        ON sh.file_id = f.file_id
    JOIN episodes AS e
        ON f.episode_id = e.episode_id
    JOIN seasons AS se
        ON e.season_id = se.season_id
    JOIN series AS s
        ON se.series_id = s.series_id
    WHERE st.status_type IN ('all', 'revision')
)
SELECT
    series,
    season_number,
    episode_number,
    MAX(
        CASE
            WHEN status_type = 'all'
            AND rn_all = 1
            THEN changed_at
        END
    ) AS all_complete_time,
    MIN(
        CASE
            WHEN status_type = 'revision'
            AND rn_revision = 1
            THEN changed_at
        END
    ) AS revision_time,
    ROUND(
        JULIANDAY(
            MIN(
                CASE
                    WHEN status_type = 'revision'
                    AND rn_revision = 1
                    THEN changed_at
                END
            )
        ) -
        JULIANDAY(
            MAX(
                CASE
                    WHEN status_type = 'all'
                    AND rn_all = 1
                    THEN changed_at
                END
            )
        ),
        2
    ) AS days_between
FROM episode_status_times
GROUP BY series, season_number, episode_number
HAVING all_complete_time IS NOT NULL AND revision_time IS NOT NULL
ORDER BY days_between DESC
LIMIT 10;
```

```{r, echo = FALSE}
colnames(episode_status) <- c(
  "Series",
  "Season",
  "Episode",
  "Completion Time",
  "Revision Time",
  "Days Between"
)
episode_status |> head(10) |> kable(caption = "10 Longest Time Between Episode Completion and Revision")
```

The analysis examines the time between fansub completion and first revision for all episodes that received revision treatment. Season 1 experienced an interval of 1494.73 days (approximately 4 years) between initial fansub release and first revision. The data indicates that only 24 of the 80 episodes in the dataset have undergone revision processes, demonstrating that the majority of fansub content has not received post-release refinement.

```{r, include = FALSE}
dbDisconnect(con)
```

\pagebreak

## Findings

### Production Variability and Workflow Impact

This database analysis reveals significant production inconsistencies in donghua that create unique workflow challenges for fansubbers. The data demonstrates extreme variability: 8-24 episodes per season, runtime fluctuations from 14:54 to 43:48 minutes, and non-standardized video resolutions requiring constant subtitle script adjustments. These irregularities force fansubbers to continuously adapt their workflows, unlike the standardized anime production patterns.

These inconsistencies have measurable impacts on translator workload: subtitle line counts can double or halve between episodes, and resolution changes demand technical reconfiguration for each release. The resource-intensive nature of donghua fansubbing is further evidenced by status tracking data showing that episodes often take years to receive revisions. Managing such complex and unpredictable workflows requires systematic data organization and analysis.

### Database Solution

Despite the complexity, relational database management proves valuable for tracking these multifaceted workflows. The normalized schema successfully handles irregular production patterns while maintaining data integrity through proper foreign key constraints. Advanced SQL queries reveal production insights that would be difficult to extract manually. While a relational database might seem excessive for fansubbing projects, the workflow irregularities documented here justify this approach, especially given that machine learning tools are already prevalent in the donghua fansubbing community.

### Community Implications

The data underscores why donghua fansubbing faces translator shortages compared to anime. Without official English support and given China's large domestic market, the burden falls entirely on volunteer translators managing technically demanding, unpaid work. Database tools could help streamline these workflows, potentially reducing barriers to entry for new fansubbers.

Given these challenges, consumers need greater patience with fansubbers. The donghua community needs volunteers for video encoding, multiplexing, timing, typesetting, and stylingâ€”technical skills that directly address workflow bottlenecks identified in this analysis. Collaborative support and database-driven workflow optimization may help the donghua fansubbing community thrive and bring more quality translations to international audiences.
